{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%alias nb_convert ~/bin/develtools/nbconvert createFolders.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %nb_convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import constants\n",
    "import class_constants\n",
    "# import & config logging first to prevent any sub modules from creating the root logger\n",
    "import logging\n",
    "from logging import handlers\n",
    "from logging import config\n",
    "logging.config.fileConfig(constants.logging_config, defaults={'logfile': constants.log_file} )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import time\n",
    "import ArgConfigParse\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_list(file):\n",
    "    '''read csv file `file` into a list\n",
    "    \n",
    "    Guess the CSV dialect (e.g. tsv, csv, etc.)\n",
    "    \n",
    "    Returns `list`'''\n",
    "    logging.debug(f'reading {file} to list')\n",
    "    csvFile = Path(file).expanduser().absolute()\n",
    "    file_csv = []\n",
    "    # try to figure out the dialect (csv, tsv, etc.)\n",
    "    with open(csvFile, 'r') as file:\n",
    "        dialect = csv.Sniffer().sniff(file.read(1024))\n",
    "        file.seek(0)\n",
    "        reader = csv.reader(file, dialect)\n",
    "        for row in reader:\n",
    "            file_csv.append(row)\n",
    "\n",
    "    return file_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_headers(csv_list, expected_headers=[]):\n",
    "    '''map row 0 of a csv as formatted as a list to a dictionary of expected header values'''\n",
    "    missing_headers = []\n",
    "    header_map = {}\n",
    "    \n",
    "    csvHeader = csv_list[0]\n",
    "    logging.debug('mapping headers')\n",
    "    logging.debug('checking for missing headers')\n",
    "    for each in expected_headers:\n",
    "        if each not in csvHeader:\n",
    "            missing_headers.append(each)\n",
    "            \n",
    "    if len(missing_headers) > 0:\n",
    "        logging.warning(f'missing expected headers: {missing_headers}')\n",
    "    for index, value in enumerate(csvHeader):\n",
    "        if value in expected_headers:\n",
    "            header_map[value] = index\n",
    "        \n",
    "    logging.debug('completed mapping')\n",
    "    return(header_map, missing_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_exit(e='unknown error in unknown module: BSoD!', exit_status=99, testing=False):\n",
    "\n",
    "        \n",
    "    print('\\n'*4)\n",
    "    if exit_status == 1:\n",
    "        logging.warning(f'exited before completion with exit code {exit_status}')\n",
    "        logging.warning(e)  \n",
    "    elif exit_status > 1:\n",
    "        logging.error(f'fatal error:\\n\\t{e}')\n",
    "    print(e)\n",
    "    sys.exit(exit_status)\n",
    "#     if not testing:\n",
    "#         try:\n",
    "#             sys.exit(exit_status)\n",
    "#         except SystemExit:\n",
    "#             pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gd_path():\n",
    "    def __init__(self, path=None):\n",
    "        '''google drive path class for files and directories\n",
    "        \n",
    "        Attributes:\n",
    "            path(`str`): path to google drive filestream object'''\n",
    "        self.confirmed = False\n",
    "        self._file_base = class_constants.file_base\n",
    "        self._dir_base = class_constants.dir_base\n",
    "        self.is_file = False\n",
    "#         self.exists = False\n",
    "        self.path = path\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'gd_path({self.path})'\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'{self.path}'\n",
    "    \n",
    "    @property\n",
    "    def path(self):\n",
    "        '''full local path to google dirve filestream object\n",
    "        \n",
    "        Args:\n",
    "            path(`str` or `Path`): /path/to/object\n",
    "            \n",
    "        Sets Attributes:\n",
    "            self.path: path to object\n",
    "            self.root: same as path for directories, parent directory for files\n",
    "            self.is_file: true for files and file-like objects, false for directories'''\n",
    "        return self._path\n",
    "    \n",
    "    @path.setter\n",
    "    def path(self, path):\n",
    "        if not path:\n",
    "            self._path = None \n",
    "        else:\n",
    "            self._path = Path(path)\n",
    "            if self._path.is_dir() and self._path.exists():\n",
    "                self.root = self._path\n",
    "                self.is_file = False\n",
    "#                 self.exists = True\n",
    "            if self.path.is_file() and self._path.exists():\n",
    "                self.root = self._path.parent\n",
    "                self.is_file = True\n",
    "#                 self.exists = True\n",
    "            \n",
    "            if not self._path.exists():\n",
    "                self.is_file = False\n",
    "                self.root = self._path.parent\n",
    "#                 self.exists = False\n",
    "\n",
    "#     @property\n",
    "    def exists(self):\n",
    "        root_exists = self.root.exists()\n",
    "        path_exists = self.path.exists()\n",
    "        return (root_exists and path_exists)\n",
    "                \n",
    "    @property\n",
    "    def file_id(self):\n",
    "        '''unique file id for each object (directories or file)\n",
    "        \n",
    "        Args:\n",
    "            path(`str` or `Path`): path to object; defaults to self.path\n",
    "        \n",
    "        Returns:\n",
    "            `list` of `str` containing the file id'''\n",
    "        try:\n",
    "            file_id = self.get_xattr('user.drive.id')\n",
    "        except FileNotFoundError as e:\n",
    "            logging.info(f'\\'{self.path}\\' does not appear to exist; cannot get attributes')\n",
    "            file_id = None\n",
    "        return file_id                \n",
    "\n",
    "    @property\n",
    "    def webview_link(self, confirm=True):\n",
    "        '''full webview link to object in google drive'''\n",
    "        self._webview_link = None\n",
    "        \n",
    "        file_id = None\n",
    "        self._webview_link = None\n",
    "        \n",
    "        if confirm:\n",
    "            self.confirm()\n",
    "        \n",
    "        if self.exists() and self.confirmed:\n",
    "            try:\n",
    "                file_id = self.file_id\n",
    "            except FileNotFoundError:\n",
    "                file_id = None\n",
    "\n",
    "\n",
    "            if len(file_id) < 1:\n",
    "                file_id = None\n",
    "            else:\n",
    "                file_id = file_id[0]\n",
    "\n",
    "            if not self.is_file and file_id:\n",
    "                self._webview_link = f'{self._dir_base}{file_id}'\n",
    "\n",
    "            if self.is_file and file_id:\n",
    "                self._webview_link = f'{self._file_base}{file_id}'\n",
    "            \n",
    "        return self._webview_link\n",
    "            \n",
    "    \n",
    "#     def get_children(self):\n",
    "#         if (not self.exists) or (self.is_file):\n",
    "#             self.children = []\n",
    "#             return self.children\n",
    "        \n",
    "#         if self.path:\n",
    "#             pass\n",
    "#         else:\n",
    "#             self.children = []\n",
    "#             return self.children\n",
    "\n",
    "    def get_xattr(self, attribute, path=None):\n",
    "        '''get the extended attributes of a file or directory\n",
    "        Args:\n",
    "            attribute('`str`'): attribute key to access\n",
    "\n",
    "        Returns:\n",
    "            `list` - attribute or key: attribute pairs\n",
    "            \n",
    "        Raises:\n",
    "            FileNotFoundError - file or directory does not exist\n",
    "            ChildProcessError - xattr utility exits with non-zero code \n",
    "                This is common for files that have no extended attributes or do not\n",
    "                have the requested attribute'''\n",
    "        if not path:\n",
    "            path = self.path\n",
    "        else:\n",
    "            path = Path(path).absolute()\n",
    "            \n",
    "        attributes = []\n",
    "        if not self.path.exists():\n",
    "            raise FileNotFoundError(self.path)\n",
    "\n",
    "        p = subprocess.Popen(f'xattr -p  {attribute} \"{path.resolve()}\"', shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "        for line in p.stdout.readlines():\n",
    "            attributes.append(line.decode(\"utf-8\").strip())\n",
    "    #         attributes = attributes + line.decode(\"utf-8\").strip()\n",
    "        retval = p.wait()\n",
    "        if retval != 0:\n",
    "            raise ChildProcessError(f'\"{path}\" is likely not a google filestream object: xattr exited with code {retval}')\n",
    "        return attributes    \n",
    "    \n",
    "    \n",
    "    def children(self, path=None, max_depth=1, pattern=\"*\", child_type='all'):\n",
    "        '''return list of all children matching a pattern of `child_type`\n",
    "\n",
    "        Args:\n",
    "            path(`str`): path to recurse\n",
    "            pattern(`str`): pattern to match (default \"*\")\n",
    "            child_type(`str`): \"all\" files and dirs (default); \"file\" file only; \"dir\" directory only\n",
    "\n",
    "        Returns:\n",
    "            `list`\n",
    "\n",
    "        based on: https://codereview.stackexchange.com/questions/161989/recursively-listing-files-in-python/162322'''\n",
    "        if not path:\n",
    "            path = self.path\n",
    "        output = []\n",
    "        known_child_types = {'all': os.path.exists, \n",
    "                             'file': os.path.isfile, \n",
    "                             'dir': os.path.isdir}\n",
    "        if not child_type in known_child_types.keys():\n",
    "            raise ValueError(f'\"{child_type}\" not a known type: {[k for k in known_child_types.keys()]}')\n",
    "        else:\n",
    "            func = known_child_types[child_type]\n",
    "\n",
    "        for depth in range(0, max_depth):\n",
    "            search_path = os.path.join(path, *(\"*\" * depth), pattern)\n",
    "            output.extend(filter(func, glob.iglob(search_path)))\n",
    "        return output\n",
    "    \n",
    "    def mkdir(self, path=None, parents=False, exist_ok=False, **kwargs):\n",
    "        '''create a directory using pathlib.Path().mkdir()\n",
    "        \n",
    "        Args:\n",
    "            path(`str` or `Path`): path to create\n",
    "            parents(`bool`): create parent directories - default false\n",
    "            exists_ok(`bool`): do not raise error if directory exists\n",
    "            kwargs: kwargs for pathlib.Path().mkdir()\n",
    "            \n",
    "        Returns:\n",
    "            `list` containing file_id'''\n",
    "        if not path:\n",
    "            path = self.path\n",
    "            logging.debug(f'using self.path: {path}')\n",
    "        else:\n",
    "            logging.debug(f'using supplied path: {path}')\n",
    "            \n",
    "        if path.is_file():\n",
    "            raise TypeError(f'{path} is a file')\n",
    "            \n",
    "        path = Path(path)\n",
    "            \n",
    "        path.mkdir(parents=parents, exist_ok=exist_ok, **kwargs)\n",
    "#         if self.confirm(path):\n",
    "        file_id = self.get_xattr('user.drive.id', path)\n",
    "        return file_id        \n",
    "    \n",
    "    def confirm(self, path=None):\n",
    "        '''confirm that an object has been synced over filesgtream\n",
    "        \n",
    "        Args:\n",
    "            path(`str` or `Path`): path to object; default is self.path\n",
    "        \n",
    "        Returns:\n",
    "            `list` of `str` containing the file id\n",
    "            \n",
    "        Attributes Set:\n",
    "            self.confirmed: True when object has been sent'''\n",
    "        \n",
    "        if not path:\n",
    "            path = self.path\n",
    "        file_id = self.file_id\n",
    "        \n",
    "        if file_id:\n",
    "            if 'local-' in file_id[0]:\n",
    "                self.confirmed = False\n",
    "                file_id = None\n",
    "            else:\n",
    "                self.confirmed = True\n",
    "        return file_id\n",
    "    \n",
    "    @classmethod\n",
    "    def mkchild(cls, path, create_now=False, **kwargs):\n",
    "        '''Factory - create child directory and return a gd_path object\n",
    "        \n",
    "        Args:\n",
    "            path(`pathlib.Path`): path to create\n",
    "            create_now(`bool`): True create immediately\n",
    "            kwargs: additional keyword arguments to pass on to pathlib.Path().mkdir()\n",
    "        Returns:\n",
    "            gd_path object with path set'''\n",
    "        \n",
    "        child = cls(path=path)\n",
    "        if create_now:\n",
    "            child.path.mkdir(**kwargs)\n",
    "        return child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class student_path(gd_path):\n",
    "    def __init__(self, root=None, ClassOf=None, Student_Number=None, LastFirst=None):\n",
    "        '''student directory in google drive; child class of gd_path\n",
    "        \n",
    "        Student directories are constructed from root/ClassOf-YYY/Last, First - Student_Number:\n",
    "        /Volumes/GoogleDrive/Shared drives/Cumm Drive/Cumm Folders/ClassOf-2020/Flynn, Erol - 123567\n",
    "        \n",
    "        Args:\n",
    "            root(`str`): root directory for student paths ( /Volumes/GoogleDrive/Shared drives/Cumm Drive/Cumm Folders/)\n",
    "            ClassOf(`str`): \"ClassOf-YYYY\" string representation of projected graduation year\n",
    "            LastFirst(`str`): \"Last, First\" string representation of student name\n",
    "            Student_Number(`int`): student id number\n",
    "            \n",
    "        Properties:\n",
    "            ClassOf(`str`): \"ClassOf-YYYY\" string representation of projected graduation year\n",
    "            LastFirst(`str`): \"Last, First\" string representation of student name\n",
    "            Student_Number(`int`): student id number\n",
    "            matches(`dict`):  name and webview link of directories that contain \"id_number\"\n",
    "            path_parts(`dict`): path compontents stored as dictionary keys'''\n",
    "        super(student_path, self).__init__(path=root)\n",
    "        self.matches = {}\n",
    "        self.path_parts = {'ClassOf': None, 'Student_Number': None, 'LastFirst': None}\n",
    "        self.ClassOf = ClassOf\n",
    "        self.LastFirst = LastFirst\n",
    "        self.Student_Number = Student_Number\n",
    "        \n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'student_path({self.path})'\n",
    "        \n",
    "        \n",
    "    def __str__(self):\n",
    "        return f'{self.path}'\n",
    "    \n",
    "    \n",
    "    def _set_path(self):\n",
    "        '''attempt to set the path based on the root and the path_parts once they are all set'''\n",
    "        for key in self.path_parts:\n",
    "            if not self.path_parts[key]:\n",
    "                self._path = None\n",
    "                break\n",
    "            else:\n",
    "                student_dir = f\"ClassOf-{self.path_parts['ClassOf']}/{self.path_parts['LastFirst']} - {self.path_parts['Student_Number']}\"\n",
    "                student_dir = f'{str(self.root)}/{student_dir}'\n",
    "                self._path = Path(student_dir)\n",
    "        return self._path\n",
    "            \n",
    "    \n",
    "#     @property\n",
    "#     def exists(self):\n",
    "\n",
    "    def exists(self):\n",
    "        '''check if self.path exists\n",
    "        *** overrides class method\n",
    "        \n",
    "        Returns:\n",
    "            `bool`: True if object exists on local file system'''\n",
    "        if not self._path:\n",
    "            exists = False\n",
    "        else:\n",
    "            exists = self.path.exists()\n",
    "        return exists\n",
    "    \n",
    "    def check_similar(self):\n",
    "        '''check for similarly named directories based on student id number \n",
    "        within the root/ClassOf-XXXX/ directory\n",
    "        \n",
    "        Properties Set:\n",
    "            self.matches(`dict`): dictionary of similar directories\n",
    "        Returns:\n",
    "            `bool`: True if matching directories found'''\n",
    "        similar = False\n",
    "        matches = {}\n",
    "        if self.path:\n",
    "            classof_path = f\"ClassOf-{self.path_parts['ClassOf']}\"\n",
    "            search_path = self.root/classof_path\n",
    "            glob = f\"*{self.path_parts['Student_Number']}*\"\n",
    "            for i in search_path.glob(glob):\n",
    "                match_id = self.get_xattr('user.drive.id', search_path/i)\n",
    "                if i.absolute().is_dir():\n",
    "                    url = '/'.join((self._dir_base, match_id[0]))\n",
    "                else:\n",
    "                    url = '/'.join((self._file_base, match_id[0]))\n",
    "                matches[str(i)] = url\n",
    "            self.matches = matches\n",
    "        return matches\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def ClassOf(self):\n",
    "        return self.path_parts['ClassOf']\n",
    "    \n",
    "    @ClassOf.setter\n",
    "    def ClassOf(self, ClassOf):\n",
    "        '''string representation of projected graduation date in format: \"ClassOf-YYYY\"\n",
    "        \n",
    "        Properties Set:\n",
    "            path_parts(`dict`): dictionary of component parts of path'''\n",
    "        if not ClassOf:\n",
    "            self.path_parts['ClassOf'] = None\n",
    "        else:\n",
    "            # attempt to coerce strings from cSV file into type int\n",
    "            ClassOf = int(ClassOf)\n",
    "            if not isinstance(ClassOf, int):\n",
    "                raise TypeError('class_of must be of type `int`')\n",
    "        self.path_parts['ClassOf'] = f'ClassOf-{ClassOf}'\n",
    "        self.path_parts['ClassOf'] = ClassOf\n",
    "        self._set_path()\n",
    "        \n",
    "        \n",
    "    @property\n",
    "    def LastFirst(self):\n",
    "        return self.path_parts['LastFirst']\n",
    "    \n",
    "    @LastFirst.setter\n",
    "    def LastFirst(self, name):\n",
    "        '''string representation of \"Last, First\" names\n",
    "        \n",
    "        Properties Set:\n",
    "            path_parts(`dict`): dictionary of component parts of path'''\n",
    "        if not name:\n",
    "            self.path_parts['LastFirst'] = None\n",
    "        else:\n",
    "            if not isinstance (name, str):\n",
    "                raise TypeError('name must be of type `str`')\n",
    "        self.path_parts['LastFirst'] = name\n",
    "        self._set_path()\n",
    "        \n",
    "    @property\n",
    "    def Student_Number(self):\n",
    "        return self.path_parts['Student_Number']\n",
    "    \n",
    "    @Student_Number.setter\n",
    "    def Student_Number(self, number):\n",
    "        '''integer of student id number\n",
    "        \n",
    "        Properties Set:\n",
    "            path_parts(`dict`): dictionary of component parts of path'''\n",
    "        if not number:\n",
    "            self.path_parts['Student_Number'] = None\n",
    "        else:\n",
    "            # try to coerce number into type int\n",
    "            number = int(number)\n",
    "            if not isinstance (number, int):\n",
    "                raise TypeError('id_number must be of type `int`')\n",
    "        self.path_parts['Student_Number'] = number  \n",
    "        self._set_path()\n",
    "\n",
    "    def mkchild(self, path, create_now=False, **kwargs):\n",
    "        '''Factory - generates gd_path objects and associated paths on file system\n",
    "        **overrides student_path inherited mkchild to return a pure gd_path object\n",
    "        \n",
    "        Args:\n",
    "            path(`pathlib.Path`): path to create\n",
    "            create_now(`bool`): True create immediately\n",
    "            kwargs: additional keyword arguments to pass on to pathlib.Path().mkdir()\n",
    "            \n",
    "        Returns:\n",
    "            `gd_path` object'''\n",
    "        child = gd_path(self.path/path)\n",
    "        if create_now:\n",
    "            child.mkdir(**kwargs)\n",
    "        return child\n",
    "                                    \n",
    "    def mkdir(self, **kwargs):\n",
    "        '''make the student directory\n",
    "        **overrides base class mkdir'''\n",
    "        if not self.path:\n",
    "            raise TypeError(f'\"{type(self.path)}\"\" object is not a Path() object')\n",
    "        file_id = super(student_path, self).mkdir(self.path, exist_ok=True, parents=True, **kwargs)\n",
    "        return file_id\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data(csv_list, expected_headers, header_map):\n",
    "    '''validate list items for proper data types\n",
    "         naievely attempts to coerce strings from CSV into expected_header types\n",
    "         returns a tuple of list of rows that were successfully coerced and those\n",
    "         that could not be coerced\n",
    "    \n",
    "    Args:\n",
    "        csv_list(`list` of `list`): csv as a nested list [['h1', 'h2'], ['item', 'item2']]\n",
    "        expected_headers(`dict`): {'literal_header': type} {'ClassOf':, int, 'Name', str}\n",
    "        header_map(`dict`): map of list index for each header {'h1': 0, 'h2': 5, 'hN': i}\n",
    "        \n",
    "    Returns:\n",
    "        (`tuple` of `list`): (valid_rows, invalid_rows)\n",
    "    '''\n",
    "    valid = []\n",
    "    invalid = []\n",
    "\n",
    "    for row in csv_list[1:]:\n",
    "        good_row = True\n",
    "        for k in expected_headers.keys():\n",
    "            # test for coercable types\n",
    "            try:\n",
    "                test = expected_headers[k](row[header_map[k]])\n",
    "            except ValueError:\n",
    "#                 do_exit(f'Bad student.export: {k} contained {row[header_map[k]]}\\ncannot continue. Please try running the export again.')\n",
    "                logging.warning(f'bad row: {row}')\n",
    "                logging.warning(f'column \"{k}\" contained \"{row[header_map[k]]}\"--this should be {(expected_headers[k])}')\n",
    "                invalid.append(row)\n",
    "                good_row = False\n",
    "                break\n",
    "        if  good_row:\n",
    "            valid.append(row)\n",
    "        \n",
    "    return valid, invalid\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_handler(handler=None, new_level=None):\n",
    "    '''adjust a logging handler\n",
    "    \n",
    "    Args:\n",
    "        handler(`str`): partial string in handler name - if none, returns list of all handlers attached to root\n",
    "            '*' adjusts all handlers to new_level\n",
    "        new_level(`str`): DEBUG, INFO, WARNING, ERROR\n",
    "    \n",
    "    Returns:\n",
    "        `list`: list of handlers and levels currently set'''\n",
    "    if not handler:\n",
    "        return(logging.getLogger().handlers)\n",
    "    \n",
    "    my_handler = None    \n",
    "    for index, val in enumerate(logging.getLogger().handlers):\n",
    "        if handler == '*':\n",
    "            my_handler = logging.getLogger().handlers[index]\n",
    "        else:\n",
    "            if handler in str(val):\n",
    "                my_handler = logging.getLogger().handlers[index]\n",
    "        if my_handler:\n",
    "            logging.info(f'setting {str(my_handler)} to {new_level}')\n",
    "            my_handler.setLevel(new_level)\n",
    "        else:\n",
    "            logging.warning(f'handler: \"{handler}\" not found')\n",
    "        \n",
    "    return logging.getLogger().handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_cmdargs():\n",
    "    '''set known command line arguments, parse sys.argv\n",
    "    \n",
    "    Returns:\n",
    "        `dict`: nested dictionary of command line arguments that matches strcture of .ini file'''\n",
    "    args = ArgConfigParse.CmdArgs()\n",
    "    args.add_argument('-s', '--student_export', ignore_none=False, metavar='/path/to/student.export.csv', \n",
    "                      type=str, dest='student_export', help='Export from PowerSchool containing: LastFirst, ClassOf, Student_Number')\n",
    "\n",
    "    args.add_argument('-g', '--google_drive', ignore_none=True, metavar='/Volumes/GoogleDrive/Shared drives/ASH Cum Folders/folder/',\n",
    "                      type=str, dest='main__drive_path', help='Full path to Google Drive Shared Drive containing cumulative files')\n",
    "\n",
    "    args.add_argument('-l', '--log_level', ignore_none=True, metavar='ERROR, WARNING, INFO, DEBUG', \n",
    "                      type=str, dest='main__log_level', help='Logging level -- Default: WARNING')\n",
    "\n",
    "    args.parse_args()\n",
    "    return args.nested_opts_dict                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_config(files):\n",
    "    '''parse .ini files \n",
    "    \n",
    "    Args:\n",
    "        files(`list`): list of `str` containing files in .ini format to parse\n",
    "    \n",
    "    Returns:\n",
    "        `dict`: nested dict of configuration'''\n",
    "    parser = ArgConfigParse.ConfigFile(config_files=files, ignore_missing=True)\n",
    "    parser.parse_config()\n",
    "    \n",
    "    return parser.config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_drive_path(drive_path=None):\n",
    "    '''check that path is a valid google drive path and contains the appropriate sentry file\n",
    "    \n",
    "    Args:\n",
    "        drive_path(`str`): path to google drive containg cummulative folders and sentry file\n",
    "    \n",
    "    Retruns:\n",
    "        `tuple` of `bool`, `str`: When true, drive is OK; when false, drive is not valid; str contains errors'''\n",
    "    # this is super redundant -- checks the following:\n",
    "    # * is a path\n",
    "    # * is a google drive path\n",
    "    # * if sentry file exists\n",
    "    # this may be a good idea considering how some users have run into many problems with this\n",
    "\n",
    "    drive_ok = True\n",
    "    msg = None\n",
    "    if not drive_path:\n",
    "        logging.info('no google drive specified')\n",
    "        drive_ok = False\n",
    "        msg = 'No Google Drive specified'\n",
    "        return drive_ok, msg\n",
    "    else:\n",
    "        drive_path = Path(drive_path)\n",
    "    \n",
    "    if not drive_path.exists():\n",
    "        logging.warning(f'specified path \"{drive_path}\" does not exist')\n",
    "        drive_ok = False\n",
    "        msg = f'The Google Drive \"{drive_path}\" does not appear to exist on Google Drive'\n",
    "        return drive_ok, msg\n",
    "    else:\n",
    "        google_drive = gd_path(drive_path)\n",
    "    \n",
    "    try:\n",
    "        google_drive.get_xattr('user.drive.id')\n",
    "    except ChildProcessError as e:\n",
    "        logging.warning(f'specified path \"{drive_path}\" is not a Google Drive path')\n",
    "        msg = f'The Google Drive \"{drive_path}\" does not appear to be a valid google Shared Drive'\n",
    "        drive_ok = False\n",
    "        return drive_ok, msg\n",
    "\n",
    "    sentry_file = constants.sentry_file    \n",
    "    sentry_file_path = drive_path/Path(sentry_file)\n",
    "    \n",
    "    if not sentry_file_path.is_file():\n",
    "        logging.warning(f'sentry file is missing in specified path \"{drive_path}\"')\n",
    "        msg = f'''The chosen google shared drive \"{drive_path}\"\n",
    "does not appear to be a Cumulative Student Folder. \n",
    "\n",
    "The file: \"{sentry_file}\" is missing. \n",
    "If you are sure {drive_path} is correct, \n",
    "please contact IT Support and askfor help. \n",
    "\n",
    "Please screenshot or copy this entire text below and provide it to IT Support.\n",
    "\n",
    "###############################################################################\n",
    "Run the command below from the terminal of the user that submitted this ticket.\n",
    "This command will create the necessary files for this script. \n",
    "\n",
    "Confirm that {drive_path} is the correct\n",
    "Google Shared Drive for Cumulative Student Folders BEFORE proceeding.\n",
    "     $ touch {drive_path}/{sentry_file}'''\n",
    "        drive_ok = False\n",
    "    \n",
    "    \n",
    "    \n",
    "    return drive_ok, msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xcreate_folders(valid_rows=[], invalid_rows=[], header_map=[], drive_path=None):\n",
    "    '''create folders in drive_path from valid_rows and header_map; validate the creation of each directory\n",
    "    \n",
    "    Args:\n",
    "        valid_rows(`list` of `list`): validated rows from CSV file to create\n",
    "        invalid_rows(`list` of `list`): invalid rows from CSV file to skip\n",
    "        header_map(`dict`): dictionary that maps headers to column in CSV\n",
    "        drive_path(`Path`): path to create folders\n",
    "    \n",
    "    Returns:\n",
    "        `dict` of `list`: dictionary containing list of student_path objects created, failed, or skipped\n",
    "    '''\n",
    "    # try to confirm created files N times before giving up\n",
    "    confirm_retry = constants.confirm_retry\n",
    "    #  wait N seconds for first try, N*retry for each subsiquent retry\n",
    "    base_wait = constants.base_wait    \n",
    "    \n",
    "    if len(valid_rows) < 1:\n",
    "        do_exit('No valid rows were found', 1)\n",
    "    if len(header_map) < 1:\n",
    "        do_exit('bad or missing header map', 1)\n",
    "        \n",
    "    if not drive_path or not isinstance(drive_path, (Path)):\n",
    "        do_exit('bad or missing drive_path', 1)\n",
    "\n",
    "    directories = {'created': [], 'skipped': [], 'invalid': invalid_rows, \n",
    "                   'confirmed': [], 'failed': []}\n",
    "    \n",
    "    # work through validated rows\n",
    "    logging.debug('processing valid rows')\n",
    "    for row in valid_rows:\n",
    "        name = row[header_map['LastFirst']]\n",
    "        class_of = row[header_map['ClassOf']]\n",
    "        id_number = row[header_map['Student_Number']]\n",
    "        \n",
    "        s_path = student_path(path=drive_path,\n",
    "                              name=name, \n",
    "                              class_of=class_of, \n",
    "                              id_number=id_number)\n",
    "        \n",
    "        # check if there already exists a directory with the student number\n",
    "        logging.debug(f'checking for similarly named folders for {name} - {id_number}')\n",
    "        if s_path.check_similar():\n",
    "            # flag those that have multiple entries\n",
    "            if len(s_path.matches) > 1:\n",
    "                logging.warning(f'multiple directories exist in {class_of} for student number {id_number}')\n",
    "                directories['skipped'].append((s_path, f'multiple: {len(s_path.matches)} existing folders found'))\n",
    "            # flag those that already exist for auditing purposes\n",
    "            else:\n",
    "                logging.info(f'skipped {class_of}/{name} - {id_number}: folder exists')\n",
    "                directories['skipped'].append((s_path, 'exists'))\n",
    "\n",
    "                \n",
    "        else:\n",
    "            # create the directory and try to handle errors as needed\n",
    "            try:\n",
    "                s_path.mkdir(parents=True)\n",
    "            except FileExistsError as e:\n",
    "                logging.error(f'{s_path.student_dir_name} exists')\n",
    "                directories['skipped'].append((s_path, 'exists'))\n",
    "            except OSError as e:\n",
    "                logging.error(f'Could not create {s_path.student_dir_name}: {e}')\n",
    "                directories['failed'].append((s_path, 'error: {e}'))\n",
    "            else:\n",
    "                directories['created'].append(s_path)\n",
    "\n",
    "# inject a bad entry to test checks at end\n",
    "#     directories['created'].append(student_path(path='/Volumes/GoogleDrive/Shared drives/IT Blabla I/spam_eggs_spam',\n",
    "#                                                name='Eggs, Green', class_of=1000, id_number=123456))\n",
    "    \n",
    "    \n",
    "    # double check that drectories were created and properly synced to google drive\n",
    "    logging.info('confirming created directories have synced')\n",
    "    for i in range(0, confirm_retry):\n",
    "        dirs_to_check = directories['created']\n",
    "        logging.debug(f'attempt {i} of {confirm_retry}')\n",
    "        logging.debug(f'{len(dirs_to_check)} directories remain to be confirmed')\n",
    "        if len(dirs_to_check) > 0:\n",
    "            wait = i * base_wait\n",
    "        for each in dirs_to_check:\n",
    "            logging.info(f'checking: {each}')\n",
    "            if each.confirm():\n",
    "                logging.debug(f'confirmed: {each}')\n",
    "                directories['confirmed'].append(each)\n",
    "                directories['created'].remove(each)\n",
    "            else:\n",
    "                logging.info(f'not confirmed')\n",
    "            \n",
    "        # loop over the created directories N times with a longer delay each time\n",
    "        # check that everything is confirmed uploaded; if it is not after Nth time, \n",
    "        # log as 'failed'\n",
    "        if len(directories['created']) > 0:\n",
    "            logging.info(f'sleeping for {wait} seconds and checking dirs again')\n",
    "            time.sleep(wait)\n",
    "        else:\n",
    "            logging.info('all created directories confirmed')\n",
    "            break\n",
    "\n",
    "    \n",
    "    # anything left in created is unconfirmed and considered failed\n",
    "    if len(directories['created']) > 0:\n",
    "        for each in directories['created']:\n",
    "            directories['failed'].append((each, 'error: could not confirm folder was created on google drive\\n\\ttry the same export again later'))\n",
    "        # zero out created\n",
    "        directories['created'] = []\n",
    "        \n",
    "        \n",
    "    return directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folders(drive_path, valid_rows, header_map):\n",
    "    grade_level_dirs = constants.student_dirs\n",
    "    \n",
    "    directories = {'created': [], 'exist': [], 'failed': [], 'multiple': [], 'subdirs': []}\n",
    "    directories_to_check = []\n",
    "\n",
    "    \n",
    "    def make_subdirs(student_dir):\n",
    "        logging.debug(f'checking grade level dirs for {student_dir}')\n",
    "        for gld in grade_level_dirs:\n",
    "            subdir = student_dir.mkchild(gld, exist_ok=True)\n",
    "            if not subdir.exists():\n",
    "                try:\n",
    "                    subdir.mkdir()\n",
    "                except (OSError, FileNotFoundError) as e:\n",
    "                    logging.warning(f'error creating grade level directory: {gld}: {e}')\n",
    "                    directories['failed'].append(subdir)\n",
    "                else:\n",
    "                    directories['subdirs'].append(subdir)\n",
    "            else:\n",
    "                if not subdir.confirm():\n",
    "                    logging.debug(f'exists, but is not confirmed: {subdir}')\n",
    "                    directories['subdirs'].append(subdir)\n",
    "#         return ok, failed\n",
    "                \n",
    "\n",
    "    \n",
    "    # build a list of directories to check\n",
    "    for student in valid_rows:\n",
    "        class_of = student[header_map['ClassOf']]\n",
    "        last_first = student[header_map['LastFirst']]\n",
    "        student_number = student[header_map['Student_Number']]\n",
    "        directories_to_check.append(student_path(dp, ClassOf=class_of, Student_Number=student_number, LastFirst=last_first))\n",
    "    \n",
    "    \n",
    "    # check for similar directories\n",
    "    for directory in directories_to_check:\n",
    "        logging.debug(f'checking: {directory}')\n",
    "        directory.check_similar()\n",
    "        \n",
    "        # new directories\n",
    "        if len(directory.matches) == 0:\n",
    "            logging.debug('\\tnew')\n",
    "            try:\n",
    "                directory.mkdir()\n",
    "            except (OSError, FileNotFoundError) as e:\n",
    "                logging.warning(f'error creating directory: {directory.path}: {e}')\n",
    "                directories['failed'].append(directory)\n",
    "            else:\n",
    "                directories['created'].append(directory)\n",
    "            \n",
    "            # queue subdirs for creation\n",
    "            make_subdirs(directory)\n",
    "#             for gld in grade_level_dirs:\n",
    "\n",
    "#                 subdir = directory.mkchild(gld, exist_ok=True)\n",
    "#                 logging.debug(f'\\t\\tchecking subdirs: {subdir}')\n",
    "#                 if not subdir.exists():\n",
    "\n",
    "#                     try:\n",
    "#                         subdir.mkdir()\n",
    "#                     except (OSError, FileNotFoundError) as e:\n",
    "#                         logging.warning(f'error creating grade level directory: {gld}: {e}')\n",
    "#                         directories['failed'].append(subdir)\n",
    "#                     else:\n",
    "#                         directories['subdirs'].append(subdir)\n",
    "                    \n",
    "\n",
    "        # existing directories           \n",
    "        if len(directory.matches) == 1:\n",
    "            logging.debug('\\texisting')\n",
    "            directories['exist'].append(directory)\n",
    "            # queue subdirs for creation\n",
    "            make_subdirs(directory)\n",
    "#             for gld in grade_level_dirs:\n",
    "#                 try:\n",
    "#                     subdir = directory.mkchild(gld, exist_ok=True)\n",
    "#                 except (OSError, FileNotFoundError) as e:\n",
    "#                     logging.warning(f'error creating grade level directory: {gld}: {e}')\n",
    "#                     directories['failed'].append(subdir)\n",
    "#                 else:\n",
    "#                     directories['subdirs'].append(subdir)\n",
    "            \n",
    "        # directories that have multiple matches\n",
    "        if len(directory.matches) > 1:\n",
    "            logging.warning('\\tmultiple matching directories found')\n",
    "            directories['multiple'].append(directory)\n",
    "            \n",
    "    \n",
    "    return(directories_to_check, directories)\n",
    "\n",
    " \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vr.append(['Prefect, Ford',\n",
    "  'fjaber@ash.nl',\n",
    "  'Charlie',\n",
    "  'Chaplin',\n",
    "  'x',\n",
    "  '10',\n",
    "  '1239871112',\n",
    "  '',\n",
    "  '+31633337711',\n",
    "  '+31633338808',\n",
    "  '000.000.0000',\n",
    "  '1990'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = Path('/Volumes/GoogleDrive/Shared drives/IT Blabla I/Student Cumulative Folders (AKA Student Portfolios)')\n",
    "tocheck, dirs = create_folders(dp, vr, hm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs['subdirs'][1].confirm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[5].check_similar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[5].matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(d[1].check_similar())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():    \n",
    "    # set the local logger\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # base configuration fle\n",
    "    config_file = Path(constants.config_file)\n",
    "    # user config file (~/.config/app_name/app.ini)\n",
    "    user_config_path = Path(constants.user_config_path)\n",
    "    \n",
    "    # if the user configuration file is missing set to True & create later at end\n",
    "    update_user_config = not(user_config_path.exists)\n",
    "    logging.debug(f'update_user_config: {update_user_config}')\n",
    "\n",
    "    # parse command line and config files - \n",
    "    cmd_args_dict = parse_cmdargs()\n",
    "    cfg_files_dict = read_config([constants.config_file, constants.user_config_path])\n",
    "\n",
    "    # merge the command line arguments and the config files; cmd line overwrites files\n",
    "    config = ArgConfigParse.merge_dict(cfg_files_dict, cmd_args_dict)\n",
    "\n",
    "    # adjust the logging levels if needed\n",
    "    if config['main']['log_level']:\n",
    "        ll = config['main']['log_level']\n",
    "        if ll in (['DEBUG', 'INFO', 'WARNING', 'ERROR']):\n",
    "            logging.root.setLevel(ll)\n",
    "            handlers = adjust_handler('*', ll)\n",
    "            logging.debug(f'adjusted log levels: {handlers}')\n",
    "        else:\n",
    "            logging.warning(f'unknown or invalid log_level: {ll}')\n",
    "    \n",
    "    # load file constants\n",
    "    expected_headers = constants.expected_headers    \n",
    "    student_dirs = constants.student_dirs\n",
    "        \n",
    "    # get csv_file and drive_path from the command line\n",
    "    try:\n",
    "        csv_file = Path(config['__cmd_line']['student_export'])\n",
    "    except TypeError:\n",
    "        logging.info('No student export file specified on command line')\n",
    "        csv_file = None\n",
    "        \n",
    "    # check drive path is a google drive path\n",
    "    drive_path = Path(config['main']['drive_path'])\n",
    "\n",
    "    drive_status = check_drive_path(drive_path)    \n",
    "    if not drive_status[0]:\n",
    "        do_exit(sentry_status[1], 1)\n",
    "        # consider prompting user at this point to enter a valid drive\n",
    "    \n",
    "    # read CSV into a list\n",
    "    if not csv_file:\n",
    "        do_exit('No student export CSV file specified. Exiting.', 1)\n",
    "    try:\n",
    "        csv_list = csv_to_list(csv_file)\n",
    "    except (FileNotFoundError, OSError, IOError, TypeError) as e:\n",
    "        logging.error(f'could not read csv file: {csv_file}')\n",
    "        logging.error(f'{e}')\n",
    "        do_exit(e, 1)\n",
    "    \n",
    "    # map the expecdted headers to the appropriate columns\n",
    "    header_map, missing_headers = map_headers(csv_list, expected_headers.keys())\n",
    "    \n",
    "    # error out if there are any missing headers in the export file\n",
    "    if len(missing_headers) > 0:\n",
    "        do_exit(f'{csv_file.name} is missing one or more headers:\\n\\t{missing_headers}\\nprogram cannot continue', 1)\n",
    "    \n",
    "    # validate the csv list\n",
    "    valid_rows, invalid_rows = validate_data(csv_list, expected_headers, header_map)\n",
    "\n",
    "    \n",
    "  \n",
    "\n",
    "    return valid_rows, header_map\n",
    "\n",
    "\n",
    "    \n",
    "    # create folders from valid rows and header_map\n",
    "    \n",
    "#     directories = create_folders()\n",
    "#     return directories\n",
    "    \n",
    "#     directories = create_folders(valid_rows=valid_rows, invalid_rows=invalid_rows, header_map=header_map,\n",
    "#                                 drive_path=drive_path)\n",
    "    \n",
    "#     # handle students with multiple entries\n",
    "#     multiple_entries = [i[0] for i in directories['skipped'] if i[1] =='multiple' ]\n",
    "\n",
    "#     print('created new:')\n",
    "#     for each in directories['confirmed']:\n",
    "#         print(each.webview_link)\n",
    "        \n",
    "        \n",
    "#     print('confirmed exist:')\n",
    "#     for each in directories['skipped']:\n",
    "#         print(each[0].webview_link)\n",
    "            \n",
    "#     if len()\n",
    "#         print('Students with multiple portfolio folders:')\n",
    "#         print('The students below have multiple cumulative folders. This is likely due to a student name change.')\n",
    "#         print('\\nYOU MUST PICK **ONE** FOLDER AND MOVE ALL THE STUDENT DATA INTO THAT ONE FOLDER.\\nDELETE THE OTHERS WHEN DONE.\\n\\nTHIS IS A MAJOR PROBLEM.')\n",
    "\n",
    "#         for each in multiple:\n",
    "#             if each.check_similar:\n",
    "#                 print(f'ClassOf-{each.class_of}: {each.name} - {each.id_number} has {len(each.matches)} folders:')\n",
    "#                 for folder in each.matches:\n",
    "#                     print(f'\\t{each.matches[folder]}')\n",
    "\n",
    "    \n",
    "        \n",
    "    if update_user_config:\n",
    "        try:\n",
    "            logging.info(f'updating user configuration file: {user_config_path}')\n",
    "            ArgConfigParse.write(config, user_config_path, create=True)\n",
    "        except Exception as e:\n",
    "            m = f'Error updating user configuration file: {e}'\n",
    "            do_exit(m, 1)\n",
    "\n",
    "            \n",
    "    # add summary of actions and errors\n",
    "    # create csv output for adding portfolio links into PS SIS\n",
    "    return directories\n",
    "    \n",
    "\n",
    "    # cleanup\n",
    "    # handle invalid_rows -- notify user of issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_output_csv(directories):\n",
    "    \n",
    "    output_list = []\n",
    "    issue_list = []\n",
    "    \n",
    "    total = 0\n",
    "    \n",
    "    for entry in directories['confirmed']:\n",
    "        # these are failures\n",
    "        pass\n",
    "    \n",
    "    for entry in directories['skipped']:\n",
    "        if entry[1] == 'exists':\n",
    "            output_list.append(entry)\n",
    "        if 'multiple:' in entry[1]:\n",
    "            issue_list.append(entry)\n",
    "            \n",
    "    for entry in directories['failed']:\n",
    "        issue_list.append(entry)\n",
    "\n",
    "    \n",
    "    total = total + len(issue_list)\n",
    "    total = total + len(output_list)\n",
    "        \n",
    "    if len(output_list) > 0:\n",
    "        logging.info(f'successfully created: {len(output_list)} of {total} folders')\n",
    "        print(f'\\nFolders were created or already existed for {len(output_list)} students')       \n",
    "        \n",
    "    if len(issue_list) > 0:\n",
    "        logging.info(f'failed to create or skipped: {len(issue_list)} of {total} folders')\n",
    "        print('\\nThe folders below were not created for the following reasons:')\n",
    "        for each in issue_list:\n",
    "            print(f'* ClassOf-{each[0].class_of}/{each[0].name} - {each[0].id_number}')\n",
    "            print(f'\\t{each[1]}')           \n",
    "    \n",
    "\n",
    "    multiple = [i[0] for i in issue_list if 'multiple:' in i[1]]\n",
    "    if len(multiple) > 0:\n",
    "            print('\\nStudents with multiple portfolio folders:')\n",
    "            for each in multiple:\n",
    "                if each.check_similar:\n",
    "                    print(f'* {each.name}')\n",
    "                    for folder in each.matches:\n",
    "                        print(f'\\t{each.matches[folder]}')\n",
    "            print('''YOU MUST MERGE THESE SO THERE IS ONLY FOLDER FOR EACH STUDENT. THIS IS A MAJOR PROBLEM.\n",
    "\n",
    "**Steps to resolve the above issue**\n",
    "\\t01. Choose one of the folders above. \n",
    "\\t02. Open each of the folders and move any student work into the chosen folder.\n",
    "\\t03. Manually delete the folders that are no longer needed.''')\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    f = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vr = f[0]\n",
    "hm = f[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjust_handler('*', 'DEBUG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv.append('-g')\n",
    "sys.argv.append('/Volumes/GoogleDrive/Shared drives/IT Blabla I/Student Cumulative Folders (AKA Student Portfolios)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv.append('-g')\n",
    "sys.argv.append('/xVolumes/GoogleDrive/Shared drives/IT Blabla I/Student Cumulative Folders (AKA Student Portfolios)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv.append('-s')\n",
    "# sys.argv.append('./student.export.text')\n",
    "sys.argv.append('./invalid.student.export.text')\n",
    "# sys.argv.append('./bad.student.export.text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv.append('-l')\n",
    "sys.argv.append('INFO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.argv.append('-s')\n",
    "# sys.argv.append('./student.export.csv.text')\n",
    "# # f = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.argv.append('-g')\n",
    "# sys.argv.append('/Volumes/GoogleDrive/Shared drives/IT Blabla I/Student Cumulative Folders (AKA Student Portfolios)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "portfolioCreator-alMouNtK",
   "language": "python",
   "name": "portfoliocreator-almountk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
