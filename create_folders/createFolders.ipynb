{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%alias nb_convert ~/bin/develtools/nbconvert createFolders.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nb_convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import constants\n",
    "# import class_constants\n",
    "# import & config logging first to prevent any sub modules from creating the root logger\n",
    "import logging\n",
    "from logging import handlers\n",
    "from logging import config\n",
    "logging.config.fileConfig(constants.logging_config, defaults={'logfile': constants.log_file} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import *\n",
    "\n",
    "from filestream import GoogleDrivePath, GDStudentPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "import sys\n",
    "from pathlib import Path\n",
    "# import subprocess\n",
    "import time\n",
    "import ArgConfigParse\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import PySimpleGUI as sg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_cmdargs():\n",
    "    '''set known command line arguments, parse sys.argv\n",
    "    \n",
    "    Returns:\n",
    "        `dict`: nested dictionary of command line arguments that matches strcture of .ini file'''\n",
    "    args = ArgConfigParse.CmdArgs()\n",
    "    args.add_argument('-s', '--student_export', ignore_none=False, metavar='/path/to/student.export.csv', \n",
    "                      type=str, dest='student_export', help='Export from PowerSchool containing: LastFirst, ClassOf, Student_Number')\n",
    "\n",
    "    args.add_argument('-g', '--google_drive', ignore_none=True, metavar='/Volumes/GoogleDrive/Shared drives/ASH Cum Folders/folder/',\n",
    "                      type=str, dest='main__drive_path', help='Full path to Google Drive Shared Drive containing cumulative files')\n",
    "\n",
    "    args.add_argument('-l', '--log_level', ignore_none=True, metavar='ERROR, WARNING, INFO, DEBUG', \n",
    "                      type=str, dest='main__log_level', help='Logging level -- Default: WARNING')\n",
    "    args.add_argument('-v', '--version', dest='version', action='store_true',\n",
    "                      default=False, help='Print version number and exit')\n",
    "\n",
    "    args.parse_args()\n",
    "    return args.nested_opts_dict                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_config(files):\n",
    "    '''parse .ini files \n",
    "    \n",
    "    Args:\n",
    "        files(`list`): list of `str` containing files in .ini format to parse\n",
    "    \n",
    "    Returns:\n",
    "        `dict`: nested dict of configuration'''\n",
    "    parser = ArgConfigParse.ConfigFile(config_files=files, ignore_missing=True)\n",
    "    parser.parse_config()\n",
    "    \n",
    "    return parser.config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_drive_path(drive_path=None):\n",
    "    '''check that path is a valid google drive path and contains the appropriate sentry file\n",
    "    \n",
    "    Args:\n",
    "        drive_path(`str`): path to google drive containg cummulative folders and sentry file\n",
    "    \n",
    "    Retruns:\n",
    "        `tuple` of `bool`, `str`: When true, drive is OK; when false, drive is not valid; str contains errors'''\n",
    "    # this is super redundant -- checks the following:\n",
    "    # * is a path\n",
    "    # * is a google drive path\n",
    "    # * if sentry file exists\n",
    "    # this may be a good idea considering how some users have run into many problems with this\n",
    "\n",
    "    drive_ok = True\n",
    "    msg = None\n",
    "    if not drive_path:\n",
    "        logging.info('no google drive specified')\n",
    "        drive_ok = False\n",
    "        msg = 'No Google Drive specified'\n",
    "        return drive_ok, msg\n",
    "    else:\n",
    "        drive_path = Path(drive_path)\n",
    "    \n",
    "    if not drive_path.exists():\n",
    "        logging.warning(f'specified path \"{drive_path}\" does not exist')\n",
    "        drive_ok = False\n",
    "        msg = f'The Google Drive \"{drive_path}\" does not appear to exist on Google Drive'\n",
    "        return drive_ok, msg\n",
    "    else:\n",
    "        google_drive = GoogleDrivePath(drive_path)\n",
    "    \n",
    "    try:\n",
    "        google_drive.get_xattr('user.drive.id')\n",
    "    except ChildProcessError as e:\n",
    "        logging.warning(f'specified path \"{drive_path}\" is not a Google Drive path')\n",
    "        msg = f'The Google Drive \"{drive_path}\" does not appear to be a valid google Shared Drive'\n",
    "        drive_ok = False\n",
    "        return drive_ok, msg\n",
    "\n",
    "    sentry_file = constants.sentry_file    \n",
    "    sentry_file_path = drive_path/Path(sentry_file)\n",
    "    \n",
    "    if not sentry_file_path.is_file():\n",
    "        logging.warning(f'sentry file is missing in specified path \"{drive_path}\"')\n",
    "        msg = f'''The chosen google shared drive \"{drive_path}\"\n",
    "does not appear to be a Cumulative Student Folder. \n",
    "\n",
    "The file: \"{sentry_file}\" is missing. \n",
    "If you are sure {drive_path} is correct, \n",
    "please contact IT Support and askfor help. \n",
    "\n",
    "Please screenshot or copy this entire text below and provide it to IT Support.\n",
    "\n",
    "###############################################################################\n",
    "Run the command below from the terminal of the user that submitted this ticket.\n",
    "This command will create the necessary files for this script. \n",
    "\n",
    "Confirm that {drive_path} is the correct\n",
    "Google Shared Drive for Cumulative Student Folders BEFORE proceeding.\n",
    "     $ touch {drive_path}/{sentry_file}'''\n",
    "        drive_ok = False\n",
    "    \n",
    "    \n",
    "    \n",
    "    return drive_ok, msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folders(drive_path, valid_rows, header_map):\n",
    "    logging.info(f'creating folders as needed in {drive_path}')\n",
    "    grade_level_dirs = constants.student_dirs\n",
    "    \n",
    "    directories = {'created': [], 'exist': [], 'duplicate': [], 'failed': [], 'multiple': [], 'subdirs': []}\n",
    "    directories_to_check = []\n",
    "\n",
    "    \n",
    "    def make_subdirs(student_dir):\n",
    "        '''helper function to create multiple child directories in `student_dir`\n",
    "        \n",
    "        Args:\n",
    "            student_dir(`GDStudent`): parent for child directories\n",
    "        \n",
    "        Returns:\n",
    "            None'''\n",
    "        logging.debug(f'checking grade level dirs for {student_dir}')\n",
    "        for gld in grade_level_dirs:\n",
    "            subdir = student_dir.mkchild(gld, exist_ok=True)\n",
    "            if not subdir.exists():\n",
    "                try:\n",
    "                    subdir.mkdir()\n",
    "                except (OSError, FileNotFoundError) as e:\n",
    "                    logging.warning(f'error creating grade level directory: {gld}: {e}')\n",
    "                    directories['failed'].append(subdir)\n",
    "                else:\n",
    "                    directories['subdirs'].append(subdir)\n",
    "            else:\n",
    "                if not subdir.confirm():\n",
    "                    logging.debug(f'exists, but is not confirmed: {subdir}')\n",
    "                    directories['subdirs'].append(subdir)\n",
    "#         return ok, failed\n",
    "                \n",
    "    \n",
    "\n",
    "    \n",
    "    # build a list of GDStudentPath objects to check for existence/creation\n",
    "    logging.info(f'processing {len(valid_rows)} rows')\n",
    "    for student in valid_rows:\n",
    "        class_of = student[header_map['ClassOf']]\n",
    "        last_first = student[header_map['LastFirst']]\n",
    "        student_number = student[header_map['Student_Number']]\n",
    "        logging.debug(f'class: {class_of}, lastfirst: {last_first}, student number: {student_number}')\n",
    "        directories_to_check.append(GDStudentPath(drive_path, ClassOf=class_of, Student_Number=student_number, LastFirst=last_first))\n",
    "    \n",
    "    \n",
    "    # check for similar directories\n",
    "    for directory in directories_to_check:\n",
    "        logging.debug(f'checking for existing dirs with student number: {directory.Student_Number}')\n",
    "        directory.check_similar()\n",
    "        \n",
    "        # new directories\n",
    "        if len(directory.matches) == 0:\n",
    "            logging.debug(f'creating new directory for {directory.LastFirst}')\n",
    "            try:\n",
    "                directory.mkdir()\n",
    "            except (OSError, FileNotFoundError) as e:\n",
    "                logging.warning(f'error creating directory: {directory.path}: {e}')\n",
    "                directories['failed'].append(directory)\n",
    "            else:\n",
    "                directories['created'].append(directory)\n",
    "            \n",
    "            # queue subdirs for creation\n",
    "            make_subdirs(directory)\n",
    "                    \n",
    "        # existing directories           \n",
    "        if len(directory.matches) == 1 and not directory.duplicate:\n",
    "            logging.debug(f'existing directory for {directory.LastFirst}, this is OK')\n",
    "            directories['exist'].append(directory)\n",
    "            # queue subdirs for creation\n",
    "            make_subdirs(directory)\n",
    "        \n",
    "        if len(directory.matches) == 1 and directory.duplicate:\n",
    "            logging.warning(f'a directory already exists with a different LastFirst, but the same Student_Number; this is NOT OK')\n",
    "            logging.info('this directory will not be created')\n",
    "            directories['duplicate'].append(directory)\n",
    "            \n",
    "    \n",
    "        # directories that have multiple matches\n",
    "        if len(directory.matches) > 1:\n",
    "            logging.warning(f'{len(directory.matches)} existing directories found for {directory.LastFirst}; this is NOT OK')            \n",
    "            logging.info('this directory will not be created')            \n",
    "            directories['multiple'].append(directory) \n",
    "                \n",
    "    return directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_folders(directories):\n",
    "    '''Verify that processed rows have synchronized over filestream\n",
    "    report on those that have failed to sync\n",
    "    \n",
    "    Args:\n",
    "        directories(`dict`): {'created': [], 'subdirs': [], 'exist': []}\n",
    "            all other keys will be returned in the unconfirmed_sets of the tuple\n",
    "        \n",
    "    Returns:\n",
    "        tuple(confirmed_sets, unconfirmed_sets)'''\n",
    "    # try to confirm created files N times before giving up\n",
    "    confirm_retry = constants.confirm_retry\n",
    "    #  wait N seconds for first try, N*retry for each subsiquent retry\n",
    "    base_wait = constants.base_wait  \n",
    "    base_wait = 4\n",
    "    checks_complete = False\n",
    "    \n",
    "    sets_to_check = ['created', 'subdirs', 'exist']\n",
    "    \n",
    "    unconfirmed_sets = {}\n",
    "    confirmed_sets = {}    \n",
    "    \n",
    "    for each_set in directories:\n",
    "        if not each_set in sets_to_check:\n",
    "            unconfirmed_sets[each_set] = set(directories[each_set])\n",
    "    \n",
    "    for each_set in sets_to_check:\n",
    "        unconfirmed_sets[each_set] = set(directories[each_set])\n",
    "        confirmed_sets[each_set] = set()\n",
    "\n",
    "\n",
    "    for i in range(0, confirm_retry):        \n",
    "        logging.info(f'checking student directories: attempt {i+1} of {confirm_retry}')\n",
    "        unconfirmed_dir_total = 0\n",
    "        delay = base_wait * i\n",
    "        if i > 0:\n",
    "            logging.info(f'pausing {delay} seconds before checking again ')\n",
    "            time.sleep(delay)\n",
    "        \n",
    "        for each_set in sets_to_check:\n",
    "            logging.debug(f'verifying set: {each_set}')\n",
    "            confirmed_dirs = set()\n",
    "            for each_dir in unconfirmed_sets[each_set]:\n",
    "                logging.debug(f'checking {each_dir}')\n",
    "                if each_dir.confirm():\n",
    "                    logging.debug('confirmed')\n",
    "                    confirmed_dirs.add(each_dir)\n",
    "            unconfirmed_sets[each_set].difference_update(confirmed_dirs)\n",
    "            confirmed_sets[each_set].update(confirmed_dirs)\n",
    "        \n",
    "        for each_set in sets_to_check:\n",
    "            unconfirmed_dir_total = unconfirmed_dir_total + len(unconfirmed_sets[each_set])\n",
    "        logging.debug(f'{unconfirmed_dir_total} directories remain unconfirmed')\n",
    "        \n",
    "        if unconfirmed_dir_total <= 0:\n",
    "            break\n",
    "            \n",
    "    return confirmed_sets, unconfirmed_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv(confirmed, unconfirmed, invalid_list, csv_output_path=None):\n",
    "    '''write out processed directories to a CSV file for import into PowerSchool SIS\n",
    "    \n",
    "    Args:\n",
    "        confirmed(`dict` of `set`): directories that were created and confirmed to exist\n",
    "        unconfirmed(`dict` of `set`): directories that were created and could not be confirmed\n",
    "        invlaid_list('list'): rows from imported csv that contained bad data types \n",
    "        output_path(`str`): override default directory pulled from constants.csv_output_path \n",
    "            to use for output of CSV file\n",
    "        \n",
    "    Returns:\n",
    "        `tuple` of path to confirmed and unconfirmed csv files'''\n",
    "    \n",
    "    \n",
    "    \n",
    "    if csv_output_path:\n",
    "        csv_output_path = Path(csv_output_path)\n",
    "    else:\n",
    "        csv_output_path = constants.csv_output_path\n",
    "        \n",
    "        \n",
    "    if not csv_output_path.exists():\n",
    "        logging.info(f'creating output directory: {csv_output_path}')\n",
    "        try:\n",
    "            csv_output_path.mkdir(parents=True, exist_ok=True)\n",
    "        except Exception as e:\n",
    "            do_exit(e, 1)\n",
    "        \n",
    "    date = datetime.now().strftime(constants.date_format)\n",
    "    \n",
    "    confirmed_file = f'{constants.csv_output_name}'.format(date=date)\n",
    "    unconfirmed_file = f'{constants.csv_error_name}'.format(date=date)\n",
    "    invalid_file = f'{constants.csv_invalid_name}'.format(date=date)\n",
    "    \n",
    "    # handle confirmed directories\n",
    "    csv_output_headers = constants.csv_output_headers\n",
    "    csv_output_list = []\n",
    "    \n",
    "    confirmed_dirs = confirmed['created']\n",
    "    confirmed_dirs.update(confirmed['exist'])\n",
    "    \n",
    "    logging.info('processing confirmed directories')\n",
    "    # add the headers to the list\n",
    "    csv_output_list.append(list(csv_output_headers.keys()))\n",
    "    for each_dir in confirmed_dirs:\n",
    "        row = []\n",
    "        for column in csv_output_headers:\n",
    "            # retreive a property from an object using a string\n",
    "            # https://stackoverflow.com/qu            \n",
    "            object_property = getattr(each_dir, column)\n",
    "            # create a formatted string based on prop value and formatter from headers dict\n",
    "            formatted_string = f'{csv_output_headers[column]}'.format(val=object_property)\n",
    "            row.append(formatted_string)\n",
    "        csv_output_list.append(row)\n",
    "    try:\n",
    "        csv_writer(csv_output_list, csv_output_path/confirmed_file)\n",
    "    except Exception as e:\n",
    "        logging.warning(f'{e}')    \n",
    "        \n",
    "    # handle unconfirmed directories\n",
    "    csv_error_headers = constants.csv_error_headers\n",
    "    csv_error_strings = constants.csv_error_strings\n",
    "    csv_error_list = []\n",
    "\n",
    "    # add the headers to the output file\n",
    "    csv_error_list.append(list(csv_error_headers.keys()))\n",
    "\n",
    "    for each_set in unconfirmed:\n",
    "        if each_set in csv_error_strings:\n",
    "            logging.debug(f'processing unconfirmed set: {each_set}')\n",
    "            for each_dir in unconfirmed[each_set]:\n",
    "                logging.debug(f'processing: {each_dir}')\n",
    "                row = []\n",
    "                for column in csv_error_headers:\n",
    "                    try:\n",
    "                        object_property = getattr(each_dir, column)\n",
    "                    except AttributeError:\n",
    "                        formatted_string = csv_error_strings[each_set]\n",
    "                    else:\n",
    "                        formatted_string = f'{csv_error_headers[column]}'.format(val=object_property)\n",
    "                    row.append(formatted_string)\n",
    "                csv_error_list.append(row)\n",
    "        else:\n",
    "            # handle rows that are do not have a key\n",
    "            logging.warning(f'unknown set type in unconfirmed set: {each_set}')   \n",
    "    if len(csv_error_list) > 1:\n",
    "        logging.info('writing unconfirmed dirs csv')\n",
    "        try:\n",
    "            csv_writer(csv_error_list, csv_output_path/unconfirmed_file)\n",
    "        except Exception as e:\n",
    "            logging.warning(f'{e}')    \n",
    "    else:\n",
    "        logging.info('no unconfirmed directories found; no error output needed')\n",
    "        \n",
    "    #handle invalid rows that could not be processed due to bad or missing data\n",
    "    if len(invalid_list) > 1:\n",
    "        logging.info('writing invalid rows csv')\n",
    "        try:\n",
    "            csv_writer(invalid_list, csv_output_path/invalid_file)\n",
    "        except Exception as e:\n",
    "                logging.warning(f'{e}')\n",
    "\n",
    "    \n",
    "    return {'confirmed': csv_output_path/confirmed_file, \n",
    "            'unconfirmed': csv_output_path/unconfirmed_file, \n",
    "            'invalid': csv_output_path/invalid_file}\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_drive_path():\n",
    "    drive_path = sg.Window(constants.app_name,\n",
    "                          [[sg.Text ('Choose the Google Shared Drive and folder that contains student cummulative folders.')],\n",
    "                                     [sg.In(), sg.FolderBrowse()],\n",
    "                                     [sg.Ok(), sg.Cancel()]]).read(close=True)[1][0]\n",
    "    return Path(drive_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():    \n",
    "    # set the local logger\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.info('*'*50+'\\n')\n",
    "\n",
    "    if sys.argv == 1:\n",
    "        run_gui = True\n",
    "    else:\n",
    "        run_gui = True\n",
    "    \n",
    "    \n",
    "    ##### REMOVE THIS!\n",
    "    if '-f' in sys.argv:\n",
    "        logging.warning('Likely in jupyter environment -- remove this!')\n",
    "        run_gui = True\n",
    "    \n",
    "    # base configuration fle\n",
    "    config_file = Path(constants.config_file)\n",
    "    # user config file (~/.config/app_name/app.ini)\n",
    "    user_config_path = Path(constants.user_config_path)\n",
    "    \n",
    "    # if the user configuration file is missing set to True & create later at end\n",
    "    update_user_config = not(user_config_path.exists)\n",
    "    logging.debug(f'user config will be created: {update_user_config}')\n",
    "\n",
    "    # parse command line and config files - \n",
    "    cmd_args_dict = parse_cmdargs()\n",
    "    cfg_files_dict = read_config([constants.config_file, constants.user_config_path])\n",
    "\n",
    "    # merge the command line arguments and the config files; cmd line overwrites files\n",
    "    config = ArgConfigParse.merge_dict(cfg_files_dict, cmd_args_dict)\n",
    "    \n",
    "#     return config\n",
    "    \n",
    "    # launch a window for monitoring stdout if run_gui\n",
    "    if run_gui:\n",
    "        sg.Print('Re-routing the stdout', do_not_reroute_stdout=False)\n",
    "        pass\n",
    "    \n",
    "    # get drive_path through gui if needed\n",
    "    if not config['main']['drive_path'] and run_gui:\n",
    "        logging.debug('launching GUI folder browser')\n",
    "        ret_drive_path = window_drive_path()\n",
    "        if not ret_drive_path:\n",
    "            do_exit('You must specify a Google Shared drive to proceed.', 1)\n",
    "        else:\n",
    "            config['main']['drive_path'] = ret_drive_path\n",
    "            update_user_config = True\n",
    "\n",
    "    if config['__cmd_line']['version']:\n",
    "        print(f'{constants.app_name} version:{constants.version}')\n",
    "        print(f'Developer contact information:\\n\\t{constants.contact}\\n\\t{constants.git_repo}')\n",
    "        do_exit('', 0)\n",
    "\n",
    "    # adjust the logging levels if needed\n",
    "    if config['main']['log_level']:\n",
    "        ll = config['main']['log_level']\n",
    "        if ll in (['DEBUG', 'INFO', 'WARNING', 'ERROR']):\n",
    "            logging.root.setLevel(ll)\n",
    "            handlers = adjust_handler('*', ll)\n",
    "            logging.debug(f'adjusted log levels: {handlers} to {ll}')\n",
    "        else:\n",
    "            logging.warning(f'unknown or invalid log_level: {ll}')\n",
    "    \n",
    "    # load file constants\n",
    "    expected_headers = constants.expected_headers    \n",
    "    student_dirs = constants.student_dirs\n",
    "        \n",
    "    # get csv_file and drive_path from the command line\n",
    "    try:\n",
    "        csv_file = Path(config['__cmd_line']['student_export'])\n",
    "    except TypeError:\n",
    "        logging.info('No student export file specified on command line')\n",
    "        csv_file = None\n",
    "        \n",
    "    # check drive path is a google drive path\n",
    "    drive_path = Path(config['main']['drive_path'])\n",
    "\n",
    "    drive_status = check_drive_path(drive_path)    \n",
    "    if not drive_status[0]:\n",
    "        do_exit(drive_status[1], 1)\n",
    "        # consider prompting user at this point to enter a valid drive\n",
    "    \n",
    "    if not csv_file and run_gui:\n",
    "        csv_file = Path(sg.popup_get_file('Select a Student Export File to Process'))\n",
    "    \n",
    "    # read CSV into a list\n",
    "    if not csv_file:\n",
    "        do_exit('No student export CSV file specified. Exiting.', 1)\n",
    "    try:\n",
    "        csv_list = csv_to_list(csv_file)\n",
    "    except (FileNotFoundError, OSError, IOError, TypeError) as e:\n",
    "        logging.error(f'could not read csv file: {csv_file}')\n",
    "        logging.error(f'{e}')\n",
    "        do_exit(e, 1)\n",
    "    except csv.Error as e:\n",
    "        logging.error(f'e')\n",
    "        do_exit(f'{e} of file \"{csv_file}\". Make sure to use the field delimter \"Comma\" when preparing the export', 1)\n",
    "    \n",
    "    # map the expecdted headers to the appropriate columns\n",
    "    header_map, missing_headers = map_headers(csv_list, expected_headers.keys())\n",
    "    \n",
    "    # error out if there are any missing headers in the export file\n",
    "    if len(missing_headers) > 0:\n",
    "        do_exit(f'{csv_file.name} is missing one or more headers:\\n\\t{missing_headers}\\nprogram cannot continue', 1)\n",
    "    \n",
    "    # validate the csv list\n",
    "    valid_rows, invalid_rows = validate_data(csv_list, expected_headers, header_map)\n",
    "    \n",
    "    # insert the headers to the invalid rows list for output later\n",
    "    invalid_rows.insert(0, csv_list[0])\n",
    "    \n",
    "#     return valid_rows, invalid_rows, header_map\n",
    "    \n",
    "    directories = create_folders(drive_path=drive_path, valid_rows=valid_rows, header_map=header_map)\n",
    "    \n",
    "    confirmed_dirs, unconfirmed_dirs = check_folders(directories)\n",
    "    \n",
    "#     return confirmed_dirs, unconfirmed_dirs\n",
    "\n",
    "    csv_files = write_csv(confirmed_dirs, unconfirmed_dirs, invalid_rows)\n",
    "        \n",
    "    if update_user_config:\n",
    "        try:\n",
    "            logging.info(f'updating user configuration file: {user_config_path}')\n",
    "            ArgConfigParse.write(config, user_config_path, create=True)\n",
    "        except Exception as e:\n",
    "            m = f'Error updating user configuration file: {e}'\n",
    "            do_exit(m, 1)\n",
    "    \n",
    "    \n",
    "    len_confirmed = len_of_dict(confirmed_dirs)\n",
    "    len_unconfirmed = len_of_dict(unconfirmed_dirs)\n",
    "            \n",
    "    # Add a summary output:\n",
    "    print('********* Summary **********')\n",
    "    print(f'Processed {len(csv_list)-1} student records from \"{csv_file}\"')\n",
    "    print(f'{len(valid_rows)} records contained valid data and were processed.')\n",
    "    if len(invalid_rows) > 1:\n",
    "        print(f'{len(invalid_rows)-1} records contained invalid data and could not be used')\n",
    "    print(f'\\n')\n",
    "\n",
    "    if len_confirmed > 0:\n",
    "        print(f'Succesfully created or validated folders are stored in: \\n{csv_files[\"confirmed\"]}\\n\\tShare this file with the PowerSchool Administrator\\n')\n",
    "    if len_unconfirmed > 0:\n",
    "        print(f'Records that could not be confirmed are stored in: \\n{csv_files[\"unconfirmed\"]}\\n\\tPlease run the tool again')\n",
    "    if len(invalid_rows) > 1:\n",
    "        print(f'Rows that contained invalid data that were NOT processed are stored in: \\n{csv_files[\"invalid\"]}\\n\\tReview this file to learn more.')  \n",
    "    \n",
    "    logging.debug('done')\n",
    "    # final print\n",
    "    print('.')\n",
    "    return valid_rows, invalid_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    if '-f' in sys.argv:\n",
    "        print = sg.Print\n",
    "        print('running gui')\n",
    "    f = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust_handler('*', 'DEBUG')\n",
    "# adjust_handler('*', 'INFO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sys.argv.append('-g')\n",
    "# # sys.argv.append('/Volumes/GoogleDrive/Shared drives/IT Blabla I/Student Cumulative Folders (AKA Student Portfolios)')\n",
    "\n",
    "# # # sys.argv.append('-g')\n",
    "# # # sys.argv.append('/xVolumes/GoogleDrive/Shared drives/IT Blabla I/Student Cumulative Folders (AKA Student Portfolios)')\n",
    "\n",
    "# sys.argv.append('-s')\n",
    "# # sys.argv.append('./data/student.export.text')\n",
    "# sys.argv.append('./data/invalid.student.export.text')\n",
    "# # sys.argv.append('./bad.student.export.text')\n",
    "\n",
    "# sys.argv.append('-v')\n",
    "\n",
    "# # sys.argv.append('-l')\n",
    "# # sys.argv.append('INFO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.argv.pop()\n",
    "\n",
    "# # sys.argv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.argv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "portfolioCreator-alMouNtK",
   "language": "python",
   "name": "portfoliocreator-almountk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
